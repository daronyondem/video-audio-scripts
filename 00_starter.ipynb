{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import langchain as lc\n",
    "from langchain.document_loaders import SRTLoader\n",
    "from dotenv import load_dotenv\n",
    "import srt\n",
    "import codecs\n",
    "import tiktoken\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "#model_name = \"gpt-3.5-turbo-16k\"\n",
    "model_name = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating video chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:43:15.619000 Yeni İcatlar ve Roket Sayısı\n",
      "0:43:34.270000 Yapay Zeka ve İnsan İlişkisi\n",
      "0:44:00.469000 Mülakat Soruları ve Cevapları\n",
      "0:44:37.310000 Teknoloji ve İnsan Farkı\n",
      "0:45:53.290000 Siri ve Apple Güncellemeleri\n"
     ]
    }
   ],
   "source": [
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model= model_name,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def process_srt_file(file_path):\n",
    "    # Read and parse SRT file\n",
    "    with codecs.open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        file_content = f.read()\n",
    "        subtitle_generator = srt.parse(file_content)\n",
    "        subtitles = list(subtitle_generator)\n",
    "    \n",
    "    result = \"\"\n",
    "    chunk = \"\"\n",
    "    for sub in subtitles:\n",
    "        chunk += str(sub.start) + \">\" + str(sub.end.total_seconds) + \"\\\\n\" + sub.content\n",
    "        if num_tokens_from_string(chunk) > 7000:\n",
    "            prompt = \"Below is a part of a video transcript. You need to split the video into five topic chapters. The chapters will be used to navigate in the larger video timeline to let watchers switch between topics. Read the entire transcript. Once done reading, split it into chapters. Provide the list of chapters in this format [HH:MM:SS Chapter Name]. Put each chapter in a separate line in plain text. Match the transcript language in the output.\\\\n\\\\n\" + chunk\n",
    "            completion = get_completion(prompt)\n",
    "            result += completion\n",
    "            clear_output(wait=True)\n",
    "            print(completion)\n",
    "            chunk = \"\"        \n",
    "\n",
    "    return result\n",
    "\n",
    "final_output = process_srt_file(\"C:\\\\Users\\\\daronyondem\\\\Downloads\\\\2023-09-09_12.13.srt\")\n",
    "with open('chapters.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating video summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The speaker discusses the potential risks and ethical issues associated with artificial intelligence and data sharing. They mention the importance of data isolation and caution when using AI, especially in a business context. They also discuss the potential for misuse of data, such as personal or financial information, if it falls into the wrong hands. The speaker suggests that companies like Apple and Samsung need to be careful with how they use and share data.\n"
     ]
    }
   ],
   "source": [
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model= model_name,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def process_srt_file(file_path):\n",
    "    # Read and parse SRT file\n",
    "    with codecs.open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "        file_content = f.read()\n",
    "        subtitle_generator = srt.parse(file_content)\n",
    "        subtitles = list(subtitle_generator)\n",
    "    \n",
    "    result = \"\"\n",
    "    chunk = \"\"\n",
    "    for sub in subtitles:\n",
    "        chunk += str(sub.start) + \">\" + str(sub.end.total_seconds) + \"\\\\n\" + sub.content\n",
    "        if num_tokens_from_string(chunk) > 7000:\n",
    "            prompt = \"Below is a part of a video transcript. Your goal is to summarize the entire video. You need to create the shortest summary of this section as possible that you will combine with other sections to create the full summary of the entire video.\\\\n\\\\n\" + chunk\n",
    "            completion = get_completion(prompt)\n",
    "            result += completion\n",
    "            clear_output(wait=True)\n",
    "            print(completion)\n",
    "            chunk = \"\"        \n",
    "\n",
    "    return result\n",
    "\n",
    "final_output = process_srt_file(\"C:\\\\Users\\\\daronyondem\\\\Downloads\\\\2023-09-09_12.13.srt\")\n",
    "with open('summaries.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model= model_name,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def process_srt_file(file_path):\n",
    "    content = \"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    result = \"\"\n",
    "    prompt = \"Below is a series of summaries created out of different section of a video recording. The video is published on Youtube. Provide 10 TUrkish title alternatives and a single Turkish summary for the video. Both title and summary should be inviting and helpful to watchers. \\\\n\\\\n\" + content\n",
    "    result = get_completion(prompt)\n",
    "\n",
    "    return result\n",
    "\n",
    "final_output = process_srt_file(\"summaries.txt\")\n",
    "with open('title-description.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-qna-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4ee1bbf3137c7ea9420c4fd488a55642063e5739fe2a7286130d9ba47405b69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
